{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1823f56f",
   "metadata": {},
   "source": [
    "# WOE and SHAP Experiments\n",
    "\n",
    "Author: https://www.github.com/deburky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd42f6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from fastwoe import FastWoe\n",
    "from fisher_scoring import LogisticRegression\n",
    "from pygam import LinearGAM, s\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import KBinsDiscretizer, PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b49bf2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭── Fisher Scoring Logistic Regression Fit ──╮\n",
       "│                                            │\n",
       "│         Total Fisher Scoring Iterations: <span style=\"color: #00ffff; text-decoration-color: #00ffff\">8</span> │\n",
       "│         Log Likelihood: <span style=\"color: #00ffff; text-decoration-color: #00ffff\">-3506.5143</span>         │\n",
       "│         Beta 0 = intercept (bias): <span style=\"color: #00ffff; text-decoration-color: #00ffff\">True</span>    │\n",
       "│                                            │\n",
       "╰────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭── Fisher Scoring Logistic Regression Fit ──╮\n",
       "│                                            │\n",
       "│         Total Fisher Scoring Iterations: \u001b[38;5;51m8\u001b[0m │\n",
       "│         Log Likelihood: \u001b[38;5;51m-3506.5143\u001b[0m         │\n",
       "│         Beta 0 = intercept (bias): \u001b[38;5;51mTrue\u001b[0m    │\n",
       "│                                            │\n",
       "╰────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                              Fisher Scoring Logistic Regression Summary                              </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">                 Parameter </span>┃<span style=\"font-weight: bold\"> Estimate </span>┃<span style=\"font-weight: bold\"> Std. Error </span>┃<span style=\"font-weight: bold\"> Wald Statistic </span>┃<span style=\"font-weight: bold\"> P-value </span>┃<span style=\"font-weight: bold\"> Lower CI </span>┃<span style=\"font-weight: bold\"> Upper CI </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━┩\n",
       "│<span style=\"color: #00ffff; text-decoration-color: #00ffff\">          intercept (bias) </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> -2.0987  </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 0.0352     </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> -59.6740       </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 0.0000  </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> -2.1676  </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> -2.0298  </span>│\n",
       "│<span style=\"color: #00ffff; text-decoration-color: #00ffff\">         Application_Score </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 0.4923   </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 0.0335     </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 14.6929        </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 0.0000  </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 0.4267   </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 0.5580   </span>│\n",
       "│<span style=\"color: #00ffff; text-decoration-color: #00ffff\">              Bureau_Score </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 0.5365   </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 0.0332     </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 16.1671        </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 0.0000  </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 0.4714   </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 0.6015   </span>│\n",
       "│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> SP_Number_Of_Searches_L6M </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 0.6544   </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 0.0955     </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 6.8533         </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 0.0000  </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 0.4672   </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 0.8415   </span>│\n",
       "└───────────────────────────┴──────────┴────────────┴────────────────┴─────────┴──────────┴──────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                              Fisher Scoring Logistic Regression Summary                              \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m                Parameter\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mEstimate\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mStd. Error\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mWald Statistic\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mP-value\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mLower CI\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mUpper CI\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━┩\n",
       "│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m         intercept (bias)\u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m-2.0987 \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m0.0352    \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m-59.6740      \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m0.0000 \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m-2.1676 \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m-2.0298 \u001b[0m\u001b[38;5;51m \u001b[0m│\n",
       "│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m        Application_Score\u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m0.4923  \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m0.0335    \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m14.6929       \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m0.0000 \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m0.4267  \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m0.5580  \u001b[0m\u001b[38;5;51m \u001b[0m│\n",
       "│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m             Bureau_Score\u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m0.5365  \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m0.0332    \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m16.1671       \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m0.0000 \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m0.4714  \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m0.6015  \u001b[0m\u001b[38;5;51m \u001b[0m│\n",
       "│\u001b[38;5;51m \u001b[0m\u001b[38;5;51mSP_Number_Of_Searches_L6M\u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m0.6544  \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m0.0955    \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m6.8533        \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m0.0000 \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m0.4672  \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m0.8415  \u001b[0m\u001b[38;5;51m \u001b[0m│\n",
       "└───────────────────────────┴──────────┴────────────┴────────────────┴─────────┴──────────┴──────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set the path to the data directory\n",
    "data_dir = Path.cwd().parent / \"data\"\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(data_dir / \"BankCaseStudyData.csv\")\n",
    "\n",
    "# Define the features and label\n",
    "features = [\n",
    "    \"Application_Score\",\n",
    "    \"Bureau_Score\",\n",
    "    \"SP_Number_Of_Searches_L6M\",\n",
    "]\n",
    "\n",
    "label = \"Final_Decision\"\n",
    "\n",
    "X = df[features]\n",
    "y = df[label].map({\"Accept\": 0, \"Decline\": 1})\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"woe\", FastWoe(binning_method=\"tree\")),\n",
    "        (\"logistic_regression\", LogisticRegression()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "pipeline[-1].display_summary(style=\"cyan1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd84a2e",
   "metadata": {},
   "source": [
    "## Experiment 1: Baking Intercept into WOE\n",
    "\n",
    "WOE is a centered log-odds per bin. Therefore, if we want to fit logistic regression without intercept but still retain calibration, we can bake in the log-odds into the WOE scores.\n",
    "\n",
    "The transformation is simple:\n",
    "\n",
    "$$\n",
    "\\text{logit}(p_i) = \\text{logit}(p) + \\sum_{j=1}^d WOE_{ij}\n",
    "$$\n",
    "\n",
    "The we can fit a logistic regression without intercept:\n",
    "\n",
    "$$\n",
    "\\text{logit}(p_i) = \\sum_{j=1}^d \\beta_j WOE_{ij}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08aaf160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log odds: -2.15\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭── Fisher Scoring Logistic Regression Fit ──╮\n",
       "│                                            │\n",
       "│         Total Fisher Scoring Iterations: <span style=\"color: #00ffff; text-decoration-color: #00ffff\">8</span> │\n",
       "│         Log Likelihood: <span style=\"color: #00ffff; text-decoration-color: #00ffff\">-3629.8667</span>         │\n",
       "│         Beta 0 = intercept (bias): <span style=\"color: #00ffff; text-decoration-color: #00ffff\">False</span>   │\n",
       "│                                            │\n",
       "╰────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭── Fisher Scoring Logistic Regression Fit ──╮\n",
       "│                                            │\n",
       "│         Total Fisher Scoring Iterations: \u001b[38;5;51m8\u001b[0m │\n",
       "│         Log Likelihood: \u001b[38;5;51m-3629.8667\u001b[0m         │\n",
       "│         Beta 0 = intercept (bias): \u001b[38;5;51mFalse\u001b[0m   │\n",
       "│                                            │\n",
       "╰────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                              Fisher Scoring Logistic Regression Summary                              </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">                 Parameter </span>┃<span style=\"font-weight: bold\"> Estimate </span>┃<span style=\"font-weight: bold\"> Std. Error </span>┃<span style=\"font-weight: bold\"> Wald Statistic </span>┃<span style=\"font-weight: bold\"> P-value </span>┃<span style=\"font-weight: bold\"> Lower CI </span>┃<span style=\"font-weight: bold\"> Upper CI </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━┩\n",
       "│<span style=\"color: #00ffff; text-decoration-color: #00ffff\">         Application_Score </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 0.6152   </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 0.0265     </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 23.1869        </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 0.0000  </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 0.5632   </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 0.6672   </span>│\n",
       "│<span style=\"color: #00ffff; text-decoration-color: #00ffff\">              Bureau_Score </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 0.4604   </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 0.0261     </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 17.6433        </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 0.0000  </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 0.4092   </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 0.5115   </span>│\n",
       "│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> SP_Number_Of_Searches_L6M </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> -0.0938  </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 0.0182     </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> -5.1439        </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 0.0000  </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> -0.1296  </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> -0.0581  </span>│\n",
       "└───────────────────────────┴──────────┴────────────┴────────────────┴─────────┴──────────┴──────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                              Fisher Scoring Logistic Regression Summary                              \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m                Parameter\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mEstimate\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mStd. Error\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mWald Statistic\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mP-value\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mLower CI\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mUpper CI\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━┩\n",
       "│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m        Application_Score\u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m0.6152  \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m0.0265    \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m23.1869       \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m0.0000 \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m0.5632  \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m0.6672  \u001b[0m\u001b[38;5;51m \u001b[0m│\n",
       "│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m             Bureau_Score\u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m0.4604  \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m0.0261    \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m17.6433       \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m0.0000 \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m0.4092  \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m0.5115  \u001b[0m\u001b[38;5;51m \u001b[0m│\n",
       "│\u001b[38;5;51m \u001b[0m\u001b[38;5;51mSP_Number_Of_Searches_L6M\u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m-0.0938 \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m0.0182    \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m-5.1439       \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m0.0000 \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m-0.1296 \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m-0.0581 \u001b[0m\u001b[38;5;51m \u001b[0m│\n",
       "└───────────────────────────┴──────────┴────────────┴────────────────┴─────────┴──────────┴──────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1045 0.1045\n"
     ]
    }
   ],
   "source": [
    "woe_transformer = FastWoe(\n",
    "    binning_method=\"tree\",\n",
    "    tree_kwargs={\"max_depth\": 2, \"min_samples_leaf\": 5},\n",
    "    # binning_method=\"kbins\",\n",
    "    # binner_kwargs={\"n_bins\": 3, \"strategy\": \"uniform\"},\n",
    ")\n",
    "woe_transformer.fit(X_train, y_train)\n",
    "\n",
    "X_train_woe = woe_transformer.transform(X_train)\n",
    "X_test_woe = woe_transformer.transform(X_test)\n",
    "\n",
    "log_odds = np.log(y_train.mean()) - np.log(1 - y_train.mean())\n",
    "print(f\"Log odds: {log_odds:.2f}\")\n",
    "\n",
    "# Bake in log odds into WOE scores\n",
    "X_train_log_odds = X_train_woe + log_odds\n",
    "\n",
    "# Fit LR\n",
    "USE_BIAS = False\n",
    "lr_model_no_bias = LogisticRegression(use_bias=USE_BIAS)\n",
    "lr_model_no_bias.fit(X_train_log_odds, y_train)\n",
    "lr_model_no_bias.display_summary(style=\"cyan1\")\n",
    "\n",
    "# Compare probabilities\n",
    "y_pred = lr_model_no_bias.predict_proba(X_train_log_odds)[:, 1]\n",
    "print(round(y_pred.mean(), 4), round(y_train.mean(), 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de8fbd3",
   "metadata": {},
   "source": [
    "## Experiment 2: SHAP Logistic Regression\n",
    "\n",
    "Here we use SHAP values as features in the logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d8682b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini score CatBoost: 0.8827\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭── Fisher Scoring Logistic Regression Fit ──╮\n",
       "│                                            │\n",
       "│         Total Fisher Scoring Iterations: <span style=\"color: #00ffff; text-decoration-color: #00ffff\">8</span> │\n",
       "│         Log Likelihood: <span style=\"color: #00ffff; text-decoration-color: #00ffff\">-3149.7834</span>         │\n",
       "│         Beta 0 = intercept (bias): <span style=\"color: #00ffff; text-decoration-color: #00ffff\">True</span>    │\n",
       "│                                            │\n",
       "╰────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭── Fisher Scoring Logistic Regression Fit ──╮\n",
       "│                                            │\n",
       "│         Total Fisher Scoring Iterations: \u001b[38;5;51m8\u001b[0m │\n",
       "│         Log Likelihood: \u001b[38;5;51m-3149.7834\u001b[0m         │\n",
       "│         Beta 0 = intercept (bias): \u001b[38;5;51mTrue\u001b[0m    │\n",
       "│                                            │\n",
       "╰────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                              Fisher Scoring Logistic Regression Summary                              </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">                 Parameter </span>┃<span style=\"font-weight: bold\"> Estimate </span>┃<span style=\"font-weight: bold\"> Std. Error </span>┃<span style=\"font-weight: bold\"> Wald Statistic </span>┃<span style=\"font-weight: bold\"> P-value </span>┃<span style=\"font-weight: bold\"> Lower CI </span>┃<span style=\"font-weight: bold\"> Upper CI </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━┩\n",
       "│<span style=\"color: #00ffff; text-decoration-color: #00ffff\">          intercept (bias) </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> -3.0488  </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 0.0416     </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> -73.2488       </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 0.0000  </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> -3.1304  </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> -2.9673  </span>│\n",
       "│<span style=\"color: #00ffff; text-decoration-color: #00ffff\">         Application_Score </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 2.8398   </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 0.1543     </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 18.3987        </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 0.0000  </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 2.5373   </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 3.1423   </span>│\n",
       "│<span style=\"color: #00ffff; text-decoration-color: #00ffff\">              Bureau_Score </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 2.6548   </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 0.1411     </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 18.8163        </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 0.0000  </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 2.3783   </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 2.9314   </span>│\n",
       "│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> SP_Number_Of_Searches_L6M </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 1.5379   </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 0.1387     </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 11.0865        </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 0.0000  </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 1.2660   </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 1.8098   </span>│\n",
       "└───────────────────────────┴──────────┴────────────┴────────────────┴─────────┴──────────┴──────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                              Fisher Scoring Logistic Regression Summary                              \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m                Parameter\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mEstimate\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mStd. Error\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mWald Statistic\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mP-value\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mLower CI\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mUpper CI\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━┩\n",
       "│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m         intercept (bias)\u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m-3.0488 \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m0.0416    \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m-73.2488      \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m0.0000 \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m-3.1304 \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m-2.9673 \u001b[0m\u001b[38;5;51m \u001b[0m│\n",
       "│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m        Application_Score\u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m2.8398  \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m0.1543    \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m18.3987       \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m0.0000 \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m2.5373  \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m3.1423  \u001b[0m\u001b[38;5;51m \u001b[0m│\n",
       "│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m             Bureau_Score\u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m2.6548  \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m0.1411    \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m18.8163       \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m0.0000 \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m2.3783  \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m2.9314  \u001b[0m\u001b[38;5;51m \u001b[0m│\n",
       "│\u001b[38;5;51m \u001b[0m\u001b[38;5;51mSP_Number_Of_Searches_L6M\u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m1.5379  \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m0.1387    \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m11.0865       \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m0.0000 \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m1.2660  \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m1.8098  \u001b[0m\u001b[38;5;51m \u001b[0m│\n",
       "└───────────────────────────┴──────────┴────────────┴────────────────┴─────────┴──────────┴──────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1045 0.1045\n",
      "Gini score SHAP Logistic Regression: 0.8827\n"
     ]
    }
   ],
   "source": [
    "import catboost as cb\n",
    "\n",
    "cb_model = cb.CatBoostClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=5,\n",
    "    allow_writing_files=False,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "train_pool = cb.Pool(X_train, y_train)\n",
    "test_pool = cb.Pool(X_test)\n",
    "\n",
    "cb_model.fit(train_pool)\n",
    "\n",
    "shap_values = cb_model.get_feature_importance(\n",
    "    type=\"ShapValues\",\n",
    "    data=train_pool,\n",
    ")\n",
    "\n",
    "preds = cb_model.predict_proba(X_test)[:, 1]\n",
    "gini_score = roc_auc_score(y_test, preds) * 2 - 1\n",
    "print(f\"Gini score CatBoost: {gini_score:.4f}\")\n",
    "\n",
    "# Create SHAP df dropping last column (bias)\n",
    "shap_values_df = pd.DataFrame(\n",
    "    shap_values[:, :-1], index=X_train.index, columns=X_train.columns\n",
    ")\n",
    "\n",
    "lr_shap = LogisticRegression(use_bias=True)\n",
    "lr_shap.fit(shap_values_df, y_train)\n",
    "lr_shap.display_summary(style=\"cyan1\")\n",
    "\n",
    "y_pred = lr_shap.predict_proba(shap_values_df)[:, 1]\n",
    "print(round(y_pred.mean(), 4), round(y_train.mean(), 4))\n",
    "\n",
    "gini_score = roc_auc_score(y_test, preds) * 2 - 1\n",
    "print(f\"Gini score SHAP Logistic Regression: {gini_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35727a7",
   "metadata": {},
   "source": [
    "## Experiment 3: SHAP Distillation: Binning vs. GAM\n",
    "\n",
    "We approximate the **teacher model** (CatBoost) with a simpler **student**  \n",
    "(Logistic Regression) by mapping raw features into SHAP-derived features.\n",
    "\n",
    "### Teacher\n",
    "CatBoost learns a nonlinear function\n",
    "$$\n",
    "p_i^{\\text{teacher}} = \\sigma(f(x_i)),\n",
    "$$\n",
    "with SHAP decomposition\n",
    "$$\n",
    "f(x_i) \\approx \\phi_0 + \\sum_{j=1}^d \\phi_{ij}.\n",
    "$$\n",
    "\n",
    "### Student A: SHAP-Binned LR\n",
    "1. Bin raw features into intervals.  \n",
    "2. Replace each raw $x_{ij}$ by its average SHAP contribution $\\tilde{\\phi}_{ij}$.  \n",
    "3. Optionally add interaction terms $\\tilde{\\phi}_{ij}\\tilde{\\phi}_{ik}$.  \n",
    "4. Fit logistic regression:\n",
    "$$\n",
    "\\hat{p}_i = \\sigma\\!\\Big(\\beta_0 + \\sum_j \\beta_j \\tilde{\\phi}_{ij} \n",
    "+ \\sum_{j<k} \\beta_{jk}\\tilde{\\phi}_{ij}\\tilde{\\phi}_{ik}\\Big).\n",
    "$$\n",
    "\n",
    "### Student B: SHAP-GAM LR\n",
    "1. Fit a **GAM** $g_j$ such that $g_j(x_{ij}) \\approx \\phi_{ij}$.  \n",
    "2. Transform raw inputs via GAM predictions.  \n",
    "3. Expand with polynomial interactions.  \n",
    "4. Fit logistic regression on GAM-mapped SHAP features.\n",
    "\n",
    "Both students yield **linear, interpretable scorecards** that approximate  \n",
    "the teacher while preserving much of its predictive power.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6be3ee",
   "metadata": {},
   "source": [
    "### Student A: SHAP-Binned LR\n",
    "\n",
    "We distill a **teacher** (CatBoost) into a simpler **student** (Logistic Regression) using SHAP-based features.\n",
    "\n",
    "CatBoost produces SHAP values\n",
    "$$\n",
    "f(x_i) \\;\\approx\\; \\phi_0 + \\sum_{j=1}^d \\phi_{ij},\n",
    "$$\n",
    "where $\\phi_{ij}$ is the contribution of feature $j$.\n",
    "\n",
    "### Student\n",
    "1. **Bin raw features** → map bins to average SHAP contributions $\\tilde{\\phi}_{ij}$.  \n",
    "2. Optionally add interaction terms $\\tilde{\\phi}_{ij}\\tilde{\\phi}_{ik}$.  \n",
    "\n",
    "The student is then a logistic regression:\n",
    "$$\n",
    "\\hat{p}_i = \\sigma\\!\\left(\\beta_0 + \\sum_j \\beta_j \\tilde{\\phi}_{ij}\n",
    "+ \\sum_{j<k} \\beta_{jk}\\tilde{\\phi}_{ij}\\tilde{\\phi}_{ik}\\right).\n",
    "$$\n",
    "\n",
    "This keeps much of the teacher’s predictive power while remaining interpretable and deployable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9722effc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Gini: 0.8618\n",
      "Test Gini : 0.8662\n",
      "Gini score (new data): 0.9574\n"
     ]
    }
   ],
   "source": [
    "def gini_score(y_true, y_prob):\n",
    "    \"\"\"Gini score for binary classification.\"\"\"\n",
    "    return 2 * roc_auc_score(y_true, y_prob) - 1\n",
    "\n",
    "\n",
    "def fit_teacher_student(\n",
    "    X_train, y_train, X_test, y_test, n_bins=20, include_interactions=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Fit a teacher CatBoost model and a student logistic regression\n",
    "    distilled via SHAP-binned features (with optional interactions).\n",
    "    Returns artifacts for deployment.\n",
    "    \"\"\"\n",
    "\n",
    "    # Teacher\n",
    "    teacher = cb.CatBoostClassifier(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.01,\n",
    "        max_depth=6,\n",
    "        allow_writing_files=False,\n",
    "        verbose=False,\n",
    "    )\n",
    "    train_pool = cb.Pool(X_train, y_train)\n",
    "    teacher.fit(train_pool)\n",
    "\n",
    "    # SHAP values\n",
    "    shap_values = teacher.get_feature_importance(type=\"ShapValues\", data=train_pool)\n",
    "    shap_df = pd.DataFrame(\n",
    "        shap_values[:, :-1], index=X_train.index, columns=X_train.columns\n",
    "    )\n",
    "\n",
    "    # Build raw → SHAP bin mapping\n",
    "    bin_mappers = {}\n",
    "    binner = KBinsDiscretizer(n_bins=n_bins, encode=\"ordinal\", strategy=\"uniform\")\n",
    "    for col in X_train.columns:\n",
    "        X_binned = binner.fit_transform(X_train[[col]])\n",
    "        shap_avg = shap_df.groupby(X_binned.flatten())[col].mean()\n",
    "        bin_mappers[col] = {\"edges\": binner.bin_edges_[0], \"shap\": shap_avg.to_dict()}\n",
    "\n",
    "    def transform(X):\n",
    "        X_new = pd.DataFrame(index=X.index)\n",
    "        for col in X.columns:\n",
    "            edges, shap_map = bin_mappers[col][\"edges\"], bin_mappers[col][\"shap\"]\n",
    "            bins = np.digitize(X[col].values, edges[1:-1], right=False)\n",
    "            X_new[col] = [shap_map.get(b, 0.0) for b in bins]\n",
    "        return X_new\n",
    "\n",
    "    # Transform train/test\n",
    "    X_train_shap = transform(X_train)\n",
    "    X_test_shap = transform(X_test)\n",
    "\n",
    "    poly = None\n",
    "    if include_interactions:\n",
    "        poly = PolynomialFeatures(interaction_only=True, include_bias=False)\n",
    "        X_train_shap = poly.fit_transform(X_train_shap)\n",
    "        X_test_shap = poly.transform(X_test_shap)\n",
    "\n",
    "    # Student LR\n",
    "    lr_student = LogisticRegression()\n",
    "    lr_student.fit(X_train_shap, y_train)\n",
    "\n",
    "    # Predictions & Gini\n",
    "    y_train_pred = lr_student.predict_proba(X_train_shap)[:, 1]\n",
    "    y_test_pred = lr_student.predict_proba(X_test_shap)[:, 1]\n",
    "\n",
    "    return {\n",
    "        \"teacher\": teacher,\n",
    "        \"lr_student\": lr_student,\n",
    "        \"bin_mappers\": bin_mappers,\n",
    "        \"poly\": poly,\n",
    "        \"transform_func\": transform,\n",
    "        \"train_gini\": gini_score(y_train, y_train_pred),\n",
    "        \"test_gini\": gini_score(y_test, y_test_pred),\n",
    "    }\n",
    "\n",
    "\n",
    "def score_new_data(X_new, artifacts):\n",
    "    \"\"\"\n",
    "    Apply trained student model to new raw data.\n",
    "    \"\"\"\n",
    "    transform = artifacts[\"transform_func\"]\n",
    "    X_new_shap = transform(X_new)\n",
    "\n",
    "    if artifacts[\"poly\"] is not None:\n",
    "        X_new_shap = artifacts[\"poly\"].transform(X_new_shap)\n",
    "\n",
    "    return artifacts[\"lr_student\"].predict_proba(X_new_shap)[:, 1]\n",
    "\n",
    "\n",
    "# Train student\n",
    "artifacts = fit_teacher_student(\n",
    "    X_train, y_train, X_test, y_test, n_bins=30, include_interactions=True\n",
    ")\n",
    "\n",
    "print(\"Train Gini:\", round(artifacts[\"train_gini\"], 4))\n",
    "print(\"Test Gini :\", round(artifacts[\"test_gini\"], 4))\n",
    "\n",
    "X_new = X_test.sample(50, random_state=42)\n",
    "y_new = y_test.loc[X_new.index]\n",
    "y_new_pred = score_new_data(X_new, artifacts)\n",
    "\n",
    "gini_new = gini_score(y_new, y_new_pred)\n",
    "print(f\"Gini score (new data): {gini_new:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f47b777",
   "metadata": {},
   "source": [
    "### Student B: SHAP-GAM LR\n",
    "\n",
    "We approximate a complex teacher (CatBoost) using a student logistic model built on\n",
    "SHAP-like features. CatBoost logits decompose as\n",
    "\n",
    "$$\n",
    "\\text{logit}(p_i) = \\phi_0 + \\sum_{j=1}^d \\phi_{ij}\n",
    "$$\n",
    "\n",
    "where $\\phi_{ij}$ are SHAP contributions. We fit smooth GAM functions\n",
    "$g_j(x_{ij}) \\approx \\phi_{ij}$ to map raw inputs → SHAP-like effects.\n",
    "\n",
    "To recover lost interactions, we expand these GAM features with pairwise products:\n",
    "\n",
    "$$\n",
    "z_i = [g_1(x_{i1}), \\dots, g_d(x_{id}), \\; g_j(x_{ij}) g_k(x_{ik})]\n",
    "$$\n",
    "\n",
    "and train a logistic regression:\n",
    "\n",
    "$$\n",
    "\\hat{p}_i = \\sigma\\!\\left(\\beta_0 + \\sum_j \\beta_j g_j(x_{ij}) + \\sum_{j<k} \\beta_{jk} g_j(x_{ij}) g_k(x_{ik})\\right)\n",
    "$$\n",
    "\n",
    "This student closely matches teacher Gini while remaining interpretable and deployable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01f46713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher Gini: 0.8785\n",
      "Train Gini: 0.8705\n",
      "Test Gini : 0.8754\n",
      "Gini score (new data): 0.9574\n"
     ]
    }
   ],
   "source": [
    "def fit_student_from_shap(\n",
    "    X_train,\n",
    "    shap_df,\n",
    "    X_test,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    n_splines=50,\n",
    "    include_bias=False,\n",
    "):\n",
    "    \"\"\"Fit a student model from SHAP values using GAM mappings + polynomial expansion.\"\"\"\n",
    "\n",
    "    # Fit one GAM per feature\n",
    "    gam_mappers = {}\n",
    "    for col in X_train.columns:\n",
    "        gam = LinearGAM(s(0, n_splines=n_splines)).fit(\n",
    "            X_train[[col]].values, shap_df[col].values\n",
    "        )\n",
    "        gam_mappers[col] = gam\n",
    "\n",
    "    # Transform with GAM predictions\n",
    "    def transform_with_gam(X):\n",
    "        X_new = pd.DataFrame(index=X.index)\n",
    "        for col in X.columns:\n",
    "            X_new[col] = gam_mappers[col].predict(X[[col]].values)\n",
    "        return X_new\n",
    "\n",
    "    X_train_shap = transform_with_gam(X_train)\n",
    "    X_test_shap = transform_with_gam(X_test)\n",
    "\n",
    "    # Polynomial expansion\n",
    "    poly = PolynomialFeatures(include_bias=include_bias, interaction_only=True)\n",
    "    X_train_poly = poly.fit_transform(X_train_shap)\n",
    "    X_test_poly = poly.transform(X_test_shap)\n",
    "\n",
    "    # Logistic regression on expanded features\n",
    "    lr_student = LogisticRegression()\n",
    "    lr_student.fit(X_train_poly, y_train)\n",
    "\n",
    "    # Predictions & Gini\n",
    "    y_train_pred = lr_student.predict_proba(X_train_poly)[:, 1]\n",
    "    y_test_pred = lr_student.predict_proba(X_test_poly)[:, 1]\n",
    "\n",
    "    return {\n",
    "        \"gam_mappers\": gam_mappers,\n",
    "        \"poly\": poly,\n",
    "        \"lr_student\": lr_student,\n",
    "        \"train_gini\": gini_score(y_train, y_train_pred),\n",
    "        \"test_gini\": gini_score(y_test, y_test_pred),\n",
    "    }\n",
    "\n",
    "\n",
    "def score_new_data_with_shap_gam(X_new, artifacts):\n",
    "    \"\"\"\n",
    "    Apply trained student (GAM + Poly + Logistic Regression) to new raw data.\n",
    "    \"\"\"\n",
    "    # 1. Transform raw X with GAM mappings\n",
    "    X_new_shap = pd.DataFrame(index=X_new.index)\n",
    "    for col in X_new.columns:\n",
    "        X_new_shap[col] = artifacts[\"gam_mappers\"][col].predict(X_new[[col]].values)\n",
    "\n",
    "    # 2. Apply polynomial feature expansion (if used)\n",
    "    X_new_poly = artifacts[\"poly\"].transform(X_new_shap)\n",
    "\n",
    "    # 3. Predict with student logistic regression\n",
    "    return artifacts[\"lr_student\"].predict_proba(X_new_poly)[:, 1]\n",
    "\n",
    "\n",
    "# Train teacher\n",
    "teacher = cb.CatBoostClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=6,\n",
    "    allow_writing_files=False,\n",
    "    verbose=False,\n",
    ")\n",
    "teacher.fit(X_train, y_train)\n",
    "\n",
    "teacher_gini = gini_score(y_train, teacher.predict_proba(X_train)[:, 1])\n",
    "print(f\"Teacher Gini: {teacher_gini:.4f}\")\n",
    "\n",
    "# Compute SHAP values once\n",
    "shap_values = teacher.get_feature_importance(\n",
    "    type=\"ShapValues\", data=cb.Pool(X_train, y_train)\n",
    ")\n",
    "shap_df = pd.DataFrame(\n",
    "    shap_values[:, :-1], index=X_train.index, columns=X_train.columns\n",
    ")\n",
    "\n",
    "# Run student training with polynomial expansion\n",
    "results = fit_student_from_shap(\n",
    "    X_train,\n",
    "    shap_df,\n",
    "    X_test,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    n_splines=30,\n",
    ")\n",
    "\n",
    "print(\"Train Gini:\", round(results[\"train_gini\"], 4))\n",
    "print(\"Test Gini :\", round(results[\"test_gini\"], 4))\n",
    "\n",
    "# Inference on unseen data\n",
    "X_new = X_test.sample(50, random_state=42)\n",
    "y_new = y_test.loc[X_new.index]\n",
    "y_new_pred = score_new_data_with_shap_gam(X_new, results)\n",
    "\n",
    "gini_new = gini_score(y_new, y_new_pred)\n",
    "print(\"Gini score (new data):\", round(gini_new, 4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
