{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1823f56f",
   "metadata": {},
   "source": [
    "# WOE and SHAP Experiments\n",
    "\n",
    "Author: https://www.github.com/deburky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd42f6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from fastwoe import FastWoe\n",
    "from fisher_scoring import LogisticRegression\n",
    "from pygam import LinearGAM, s\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import KBinsDiscretizer, PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1b49bf2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭── Fisher Scoring Logistic Regression Fit ──╮\n",
       "│                                            │\n",
       "│         Total Fisher Scoring Iterations: <span style=\"color: #00ffff; text-decoration-color: #00ffff\">8</span> │\n",
       "│         Log Likelihood: <span style=\"color: #00ffff; text-decoration-color: #00ffff\">-3460.4293</span>         │\n",
       "│         Beta 0 = intercept (bias): <span style=\"color: #00ffff; text-decoration-color: #00ffff\">True</span>    │\n",
       "│                                            │\n",
       "╰────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭── Fisher Scoring Logistic Regression Fit ──╮\n",
       "│                                            │\n",
       "│         Total Fisher Scoring Iterations: \u001b[38;5;51m8\u001b[0m │\n",
       "│         Log Likelihood: \u001b[38;5;51m-3460.4293\u001b[0m         │\n",
       "│         Beta 0 = intercept (bias): \u001b[38;5;51mTrue\u001b[0m    │\n",
       "│                                            │\n",
       "╰────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                          Fisher Scoring Logistic Regression Summary                           </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">          Parameter </span>┃<span style=\"font-weight: bold\"> Estimate </span>┃<span style=\"font-weight: bold\"> Std. Error </span>┃<span style=\"font-weight: bold\"> Wald Statistic </span>┃<span style=\"font-weight: bold\"> P-value </span>┃<span style=\"font-weight: bold\"> Lower CI </span>┃<span style=\"font-weight: bold\"> Upper CI </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━┩\n",
       "│<span style=\"color: #00ffff; text-decoration-color: #00ffff\">   intercept (bias) </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> -2.1146  </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 0.0353     </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> -59.9044       </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 0.0000  </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> -2.1838  </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> -2.0454  </span>│\n",
       "│<span style=\"color: #00ffff; text-decoration-color: #00ffff\">  Application_Score </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 0.6389   </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 0.0267     </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 23.9613        </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 0.0000  </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 0.5866   </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 0.6911   </span>│\n",
       "│<span style=\"color: #00ffff; text-decoration-color: #00ffff\">       Bureau_Score </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 0.4159   </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 0.0261     </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 15.9254        </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 0.0000  </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 0.3647   </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 0.4670   </span>│\n",
       "│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> Number_of_Payments </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 0.8341   </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 0.0457     </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 18.2584        </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 0.0000  </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 0.7446   </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 0.9237   </span>│\n",
       "└────────────────────┴──────────┴────────────┴────────────────┴─────────┴──────────┴──────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                          Fisher Scoring Logistic Regression Summary                           \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m         Parameter\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mEstimate\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mStd. Error\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mWald Statistic\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mP-value\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mLower CI\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mUpper CI\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━┩\n",
       "│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m  intercept (bias)\u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m-2.1146 \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m0.0353    \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m-59.9044      \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m0.0000 \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m-2.1838 \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m-2.0454 \u001b[0m\u001b[38;5;51m \u001b[0m│\n",
       "│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m Application_Score\u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m0.6389  \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m0.0267    \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m23.9613       \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m0.0000 \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m0.5866  \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m0.6911  \u001b[0m\u001b[38;5;51m \u001b[0m│\n",
       "│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m      Bureau_Score\u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m0.4159  \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m0.0261    \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m15.9254       \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m0.0000 \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m0.3647  \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m0.4670  \u001b[0m\u001b[38;5;51m \u001b[0m│\n",
       "│\u001b[38;5;51m \u001b[0m\u001b[38;5;51mNumber_of_Payments\u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m0.8341  \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m0.0457    \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m18.2584       \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m0.0000 \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m0.7446  \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m0.9237  \u001b[0m\u001b[38;5;51m \u001b[0m│\n",
       "└────────────────────┴──────────┴────────────┴────────────────┴─────────┴──────────┴──────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set the path to the data directory\n",
    "data_dir = Path.cwd().parent / \"data\"\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(data_dir / \"BankCaseStudyData.csv\")\n",
    "\n",
    "# Define the features and label\n",
    "features = [\n",
    "    \"Application_Score\",\n",
    "    \"Bureau_Score\",\n",
    "    \"Number_of_Payments\",\n",
    "]\n",
    "\n",
    "label = \"Final_Decision\"\n",
    "\n",
    "X = df[features]\n",
    "y = df[label].map({\"Accept\": 0, \"Decline\": 1})\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"woe\",\n",
    "            FastWoe(\n",
    "                binning_method=\"tree\",\n",
    "                tree_kwargs={\"max_depth\": 2, \"min_samples_leaf\": 5},\n",
    "            ),\n",
    "        ),\n",
    "        (\"logistic_regression\", LogisticRegression()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "pipeline[-1].display_summary(style=\"cyan1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd84a2e",
   "metadata": {},
   "source": [
    "## Experiment 1: Baking Intercept into WOE\n",
    "\n",
    "WOE is a centered log-odds per bin. Therefore, if we want to fit logistic regression without intercept but still retain calibration, we can bake in the log-odds into the WOE scores.\n",
    "\n",
    "The transformation is simple:\n",
    "\n",
    "$$\n",
    "\\text{logit}(p_i) = \\text{logit}(p) + \\sum_{j=1}^d WOE_{ij}\n",
    "$$\n",
    "\n",
    "The we can fit a logistic regression without intercept:\n",
    "\n",
    "$$\n",
    "\\text{logit}(p_i) = \\sum_{j=1}^d \\beta_j WOE_{ij}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "08aaf160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log odds: -2.15\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭── Fisher Scoring Logistic Regression Fit ──╮\n",
       "│                                            │\n",
       "│         Total Fisher Scoring Iterations: <span style=\"color: #00ffff; text-decoration-color: #00ffff\">8</span> │\n",
       "│         Log Likelihood: <span style=\"color: #00ffff; text-decoration-color: #00ffff\">-3641.2023</span>         │\n",
       "│         Beta 0 = intercept (bias): <span style=\"color: #00ffff; text-decoration-color: #00ffff\">False</span>   │\n",
       "│                                            │\n",
       "╰────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭── Fisher Scoring Logistic Regression Fit ──╮\n",
       "│                                            │\n",
       "│         Total Fisher Scoring Iterations: \u001b[38;5;51m8\u001b[0m │\n",
       "│         Log Likelihood: \u001b[38;5;51m-3641.2023\u001b[0m         │\n",
       "│         Beta 0 = intercept (bias): \u001b[38;5;51mFalse\u001b[0m   │\n",
       "│                                            │\n",
       "╰────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                          Fisher Scoring Logistic Regression Summary                           </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">          Parameter </span>┃<span style=\"font-weight: bold\"> Estimate </span>┃<span style=\"font-weight: bold\"> Std. Error </span>┃<span style=\"font-weight: bold\"> Wald Statistic </span>┃<span style=\"font-weight: bold\"> P-value </span>┃<span style=\"font-weight: bold\"> Lower CI </span>┃<span style=\"font-weight: bold\"> Upper CI </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━┩\n",
       "│<span style=\"color: #00ffff; text-decoration-color: #00ffff\">  Application_Score </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 0.5863   </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 0.0273     </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 21.4649        </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 0.0000  </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 0.5328   </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 0.6399   </span>│\n",
       "│<span style=\"color: #00ffff; text-decoration-color: #00ffff\">       Bureau_Score </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 0.4196   </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 0.0272     </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 15.4271        </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 0.0000  </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 0.3663   </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 0.4729   </span>│\n",
       "│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> Number_of_Payments </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 0.0358   </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 0.0170     </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 2.1087         </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 0.0350  </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 0.0025   </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 0.0691   </span>│\n",
       "└────────────────────┴──────────┴────────────┴────────────────┴─────────┴──────────┴──────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                          Fisher Scoring Logistic Regression Summary                           \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m         Parameter\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mEstimate\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mStd. Error\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mWald Statistic\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mP-value\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mLower CI\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mUpper CI\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━┩\n",
       "│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m Application_Score\u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m0.5863  \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m0.0273    \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m21.4649       \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m0.0000 \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m0.5328  \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m0.6399  \u001b[0m\u001b[38;5;51m \u001b[0m│\n",
       "│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m      Bureau_Score\u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m0.4196  \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m0.0272    \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m15.4271       \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m0.0000 \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m0.3663  \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m0.4729  \u001b[0m\u001b[38;5;51m \u001b[0m│\n",
       "│\u001b[38;5;51m \u001b[0m\u001b[38;5;51mNumber_of_Payments\u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m0.0358  \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m0.0170    \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m2.1087        \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m0.0350 \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m0.0025  \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m0.0691  \u001b[0m\u001b[38;5;51m \u001b[0m│\n",
       "└────────────────────┴──────────┴────────────┴────────────────┴─────────┴──────────┴──────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.095 0.1045\n"
     ]
    }
   ],
   "source": [
    "woe_transformer = FastWoe(\n",
    "    binning_method=\"tree\",\n",
    "    tree_kwargs={\"max_depth\": 2, \"min_samples_leaf\": 5},\n",
    ")\n",
    "woe_transformer.fit(X_train, y_train)\n",
    "\n",
    "X_train_woe = woe_transformer.transform(X_train)\n",
    "X_test_woe = woe_transformer.transform(X_test)\n",
    "\n",
    "log_odds = np.log(y_train.mean()) - np.log(1 - y_train.mean())\n",
    "print(f\"Log odds: {log_odds:.2f}\")\n",
    "\n",
    "# Bake in log odds into WOE scores\n",
    "X_train_log_odds = X_train_woe + log_odds\n",
    "\n",
    "# Fit LR\n",
    "USE_BIAS = False\n",
    "lr_model_no_bias = LogisticRegression(use_bias=USE_BIAS)\n",
    "lr_model_no_bias.fit(X_train_log_odds, y_train)\n",
    "lr_model_no_bias.display_summary(style=\"cyan1\")\n",
    "\n",
    "# Compare probabilities\n",
    "y_pred = lr_model_no_bias.predict_proba(X_train_log_odds)[:, 1]\n",
    "print(round(y_pred.mean(), 4), round(y_train.mean(), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de8fbd3",
   "metadata": {},
   "source": [
    "## Experiment 2: SHAP Logistic Regression\n",
    "\n",
    "Here we use SHAP values as features in the logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8d8682b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini score CatBoost: 0.8964\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭── Fisher Scoring Logistic Regression Fit ──╮\n",
       "│                                            │\n",
       "│         Total Fisher Scoring Iterations: <span style=\"color: #00ffff; text-decoration-color: #00ffff\">8</span> │\n",
       "│         Log Likelihood: <span style=\"color: #00ffff; text-decoration-color: #00ffff\">-3277.3460</span>         │\n",
       "│         Beta 0 = intercept (bias): <span style=\"color: #00ffff; text-decoration-color: #00ffff\">True</span>    │\n",
       "│                                            │\n",
       "╰────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭── Fisher Scoring Logistic Regression Fit ──╮\n",
       "│                                            │\n",
       "│         Total Fisher Scoring Iterations: \u001b[38;5;51m8\u001b[0m │\n",
       "│         Log Likelihood: \u001b[38;5;51m-3277.3460\u001b[0m         │\n",
       "│         Beta 0 = intercept (bias): \u001b[38;5;51mTrue\u001b[0m    │\n",
       "│                                            │\n",
       "╰────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                          Fisher Scoring Logistic Regression Summary                           </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">          Parameter </span>┃<span style=\"font-weight: bold\"> Estimate </span>┃<span style=\"font-weight: bold\"> Std. Error </span>┃<span style=\"font-weight: bold\"> Wald Statistic </span>┃<span style=\"font-weight: bold\"> P-value </span>┃<span style=\"font-weight: bold\"> Lower CI </span>┃<span style=\"font-weight: bold\"> Upper CI </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━┩\n",
       "│<span style=\"color: #00ffff; text-decoration-color: #00ffff\">   intercept (bias) </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> -3.1487  </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 0.0432     </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> -72.9375       </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 0.0000  </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> -3.2333  </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> -3.0641  </span>│\n",
       "│<span style=\"color: #00ffff; text-decoration-color: #00ffff\">  Application_Score </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 2.5851   </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 0.1758     </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 14.7087        </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 0.0000  </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 2.2406   </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 2.9296   </span>│\n",
       "│<span style=\"color: #00ffff; text-decoration-color: #00ffff\">       Bureau_Score </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 2.8374   </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 0.1710     </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 16.5966        </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 0.0000  </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 2.5024   </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 3.1725   </span>│\n",
       "│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> Number_of_Payments </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 3.3534   </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 0.1195     </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 28.0668        </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 0.0000  </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 3.1192   </span>│<span style=\"color: #00ffff; text-decoration-color: #00ffff\"> 3.5875   </span>│\n",
       "└────────────────────┴──────────┴────────────┴────────────────┴─────────┴──────────┴──────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                          Fisher Scoring Logistic Regression Summary                           \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m         Parameter\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mEstimate\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mStd. Error\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mWald Statistic\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mP-value\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mLower CI\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mUpper CI\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━┩\n",
       "│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m  intercept (bias)\u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m-3.1487 \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m0.0432    \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m-72.9375      \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m0.0000 \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m-3.2333 \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m-3.0641 \u001b[0m\u001b[38;5;51m \u001b[0m│\n",
       "│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m Application_Score\u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m2.5851  \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m0.1758    \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m14.7087       \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m0.0000 \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m2.2406  \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m2.9296  \u001b[0m\u001b[38;5;51m \u001b[0m│\n",
       "│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m      Bureau_Score\u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m2.8374  \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m0.1710    \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m16.5966       \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m0.0000 \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m2.5024  \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m3.1725  \u001b[0m\u001b[38;5;51m \u001b[0m│\n",
       "│\u001b[38;5;51m \u001b[0m\u001b[38;5;51mNumber_of_Payments\u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m3.3534  \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m0.1195    \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m28.0668       \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m0.0000 \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m3.1192  \u001b[0m\u001b[38;5;51m \u001b[0m│\u001b[38;5;51m \u001b[0m\u001b[38;5;51m3.5875  \u001b[0m\u001b[38;5;51m \u001b[0m│\n",
       "└────────────────────┴──────────┴────────────┴────────────────┴─────────┴──────────┴──────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1045 0.1045\n",
      "Gini score SHAP Logistic Regression: 0.8964\n"
     ]
    }
   ],
   "source": [
    "import catboost as cb\n",
    "\n",
    "cb_model = cb.CatBoostClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=5,\n",
    "    allow_writing_files=False,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "train_pool = cb.Pool(X_train, y_train)\n",
    "test_pool = cb.Pool(X_test)\n",
    "\n",
    "cb_model.fit(train_pool)\n",
    "\n",
    "shap_values = cb_model.get_feature_importance(\n",
    "    type=\"ShapValues\",\n",
    "    data=train_pool,\n",
    ")\n",
    "\n",
    "preds = cb_model.predict_proba(X_test)[:, 1]\n",
    "gini_score = roc_auc_score(y_test, preds) * 2 - 1\n",
    "print(f\"Gini score CatBoost: {gini_score:.4f}\")\n",
    "\n",
    "# Create SHAP df dropping last column (bias)\n",
    "shap_values_df = pd.DataFrame(\n",
    "    shap_values[:, :-1], index=X_train.index, columns=X_train.columns\n",
    ")\n",
    "\n",
    "lr_shap = LogisticRegression(use_bias=True)\n",
    "lr_shap.fit(shap_values_df, y_train)\n",
    "lr_shap.display_summary(style=\"cyan1\")\n",
    "\n",
    "y_pred = lr_shap.predict_proba(shap_values_df)[:, 1]\n",
    "print(round(y_pred.mean(), 4), round(y_train.mean(), 4))\n",
    "\n",
    "gini_score = roc_auc_score(y_test, preds) * 2 - 1\n",
    "print(f\"Gini score SHAP Logistic Regression: {gini_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35727a7",
   "metadata": {},
   "source": [
    "## Experiment 3: SHAP Distillation: Binning vs. GAM\n",
    "\n",
    "We approximate the **teacher model** (CatBoost) with a simpler **student**  \n",
    "(Logistic Regression) by mapping raw features into SHAP-derived features.\n",
    "\n",
    "### Teacher\n",
    "CatBoost learns a nonlinear function\n",
    "$$\n",
    "p_i^{\\text{teacher}} = \\sigma(f(x_i)),\n",
    "$$\n",
    "with SHAP decomposition\n",
    "$$\n",
    "f(x_i) \\approx \\phi_0 + \\sum_{j=1}^d \\phi_{ij}.\n",
    "$$\n",
    "\n",
    "### Student A: SHAP-Binned LR\n",
    "1. Bin raw features into intervals.  \n",
    "2. Replace each raw $x_{ij}$ by its average SHAP contribution $\\tilde{\\phi}_{ij}$.  \n",
    "3. Optionally add interaction terms $\\tilde{\\phi}_{ij}\\tilde{\\phi}_{ik}$.  \n",
    "4. Fit logistic regression:\n",
    "$$\n",
    "\\hat{p}_i = \\sigma\\!\\Big(\\beta_0 + \\sum_j \\beta_j \\tilde{\\phi}_{ij} \n",
    "+ \\sum_{j<k} \\beta_{jk}\\tilde{\\phi}_{ij}\\tilde{\\phi}_{ik}\\Big).\n",
    "$$\n",
    "\n",
    "### Student B: SHAP-GAM LR\n",
    "1. Fit a **GAM** $g_j$ such that $g_j(x_{ij}) \\approx \\phi_{ij}$.  \n",
    "2. Transform raw inputs via GAM predictions.  \n",
    "3. Expand with polynomial interactions.  \n",
    "4. Fit logistic regression on GAM-mapped SHAP features.\n",
    "\n",
    "Both students yield **linear, interpretable scorecards** that approximate  \n",
    "the teacher while preserving much of its predictive power.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6be3ee",
   "metadata": {},
   "source": [
    "### Student A: SHAP-Binned LR\n",
    "\n",
    "We distill a **teacher** (CatBoost) into a simpler **student** (Logistic Regression) using SHAP-based features.\n",
    "\n",
    "CatBoost produces SHAP values\n",
    "$$\n",
    "f(x_i) \\;\\approx\\; \\phi_0 + \\sum_{j=1}^d \\phi_{ij},\n",
    "$$\n",
    "where $\\phi_{ij}$ is the contribution of feature $j$.\n",
    "\n",
    "### Student\n",
    "1. **Bin raw features** → map bins to average SHAP contributions $\\tilde{\\phi}_{ij}$.  \n",
    "2. Optionally add interaction terms $\\tilde{\\phi}_{ij}\\tilde{\\phi}_{ik}$.  \n",
    "\n",
    "The student is then a logistic regression:\n",
    "$$\n",
    "\\hat{p}_i = \\sigma\\!\\left(\\beta_0 + \\sum_j \\beta_j \\tilde{\\phi}_{ij}\n",
    "+ \\sum_{j<k} \\beta_{jk}\\tilde{\\phi}_{ij}\\tilde{\\phi}_{ik}\\right).\n",
    "$$\n",
    "\n",
    "This keeps much of the teacher’s predictive power while remaining interpretable and deployable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9722effc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Gini: 0.8645\n",
      "Test Gini : 0.8732\n",
      "Gini score (new data): 0.8800\n"
     ]
    }
   ],
   "source": [
    "def gini_score(y_true, y_prob):\n",
    "    \"\"\"Gini score for binary classification.\"\"\"\n",
    "    return 2 * roc_auc_score(y_true, y_prob) - 1\n",
    "\n",
    "\n",
    "def fit_teacher_student(\n",
    "    X_train, y_train, X_test, y_test, n_bins=20, include_interactions=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Fit a teacher CatBoost model and a student logistic regression\n",
    "    distilled via SHAP-binned features (with optional interactions).\n",
    "    Returns artifacts for deployment.\n",
    "    \"\"\"\n",
    "\n",
    "    # Teacher\n",
    "    teacher = cb.CatBoostClassifier(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.01,\n",
    "        max_depth=6,\n",
    "        allow_writing_files=False,\n",
    "        verbose=False,\n",
    "    )\n",
    "    train_pool = cb.Pool(X_train, y_train)\n",
    "    teacher.fit(train_pool)\n",
    "\n",
    "    # SHAP values\n",
    "    shap_values = teacher.get_feature_importance(type=\"ShapValues\", data=train_pool)\n",
    "    shap_df = pd.DataFrame(\n",
    "        shap_values[:, :-1], index=X_train.index, columns=X_train.columns\n",
    "    )\n",
    "\n",
    "    # Build raw → SHAP bin mapping\n",
    "    bin_mappers = {}\n",
    "    binner = KBinsDiscretizer(n_bins=n_bins, encode=\"ordinal\", strategy=\"uniform\")\n",
    "    for col in X_train.columns:\n",
    "        X_binned = binner.fit_transform(X_train[[col]])\n",
    "        shap_avg = shap_df.groupby(X_binned.flatten())[col].mean()\n",
    "        bin_mappers[col] = {\"edges\": binner.bin_edges_[0], \"shap\": shap_avg.to_dict()}\n",
    "\n",
    "    def transform(X):\n",
    "        X_new = pd.DataFrame(index=X.index)\n",
    "        for col in X.columns:\n",
    "            edges, shap_map = bin_mappers[col][\"edges\"], bin_mappers[col][\"shap\"]\n",
    "            bins = np.digitize(X[col].values, edges[1:-1], right=False)\n",
    "            X_new[col] = [shap_map.get(b, 0.0) for b in bins]\n",
    "        return X_new\n",
    "\n",
    "    # Transform train/test\n",
    "    X_train_shap = transform(X_train)\n",
    "    X_test_shap = transform(X_test)\n",
    "\n",
    "    poly = None\n",
    "    if include_interactions:\n",
    "        poly = PolynomialFeatures(interaction_only=True, include_bias=False)\n",
    "        X_train_shap = poly.fit_transform(X_train_shap)\n",
    "        X_test_shap = poly.transform(X_test_shap)\n",
    "\n",
    "    # Student LR\n",
    "    lr_student = LogisticRegression()\n",
    "    lr_student.fit(X_train_shap, y_train)\n",
    "\n",
    "    # Predictions & Gini\n",
    "    y_train_pred = lr_student.predict_proba(X_train_shap)[:, 1]\n",
    "    y_test_pred = lr_student.predict_proba(X_test_shap)[:, 1]\n",
    "\n",
    "    return {\n",
    "        \"teacher\": teacher,\n",
    "        \"lr_student\": lr_student,\n",
    "        \"bin_mappers\": bin_mappers,\n",
    "        \"poly\": poly,\n",
    "        \"transform_func\": transform,\n",
    "        \"train_gini\": gini_score(y_train, y_train_pred),\n",
    "        \"test_gini\": gini_score(y_test, y_test_pred),\n",
    "    }\n",
    "\n",
    "\n",
    "def score_new_data(X_new, artifacts):\n",
    "    \"\"\"\n",
    "    Apply trained student model to new raw data.\n",
    "    \"\"\"\n",
    "    transform = artifacts[\"transform_func\"]\n",
    "    X_new_shap = transform(X_new)\n",
    "\n",
    "    if artifacts[\"poly\"] is not None:\n",
    "        X_new_shap = artifacts[\"poly\"].transform(X_new_shap)\n",
    "\n",
    "    return artifacts[\"lr_student\"].predict_proba(X_new_shap)[:, 1]\n",
    "\n",
    "\n",
    "# Train student\n",
    "artifacts = fit_teacher_student(\n",
    "    X_train, y_train, X_test, y_test, n_bins=30, include_interactions=True\n",
    ")\n",
    "\n",
    "print(\"Train Gini:\", round(artifacts[\"train_gini\"], 4))\n",
    "print(\"Test Gini :\", round(artifacts[\"test_gini\"], 4))\n",
    "\n",
    "X_new = X_test.sample(100, random_state=42)\n",
    "y_new = y_test.loc[X_new.index]\n",
    "y_new_pred = score_new_data(X_new, artifacts)\n",
    "\n",
    "gini_new = gini_score(y_new, y_new_pred)\n",
    "print(f\"Gini score (new data): {gini_new:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f47b777",
   "metadata": {},
   "source": [
    "### Student B: SHAP-GAM LR\n",
    "\n",
    "We approximate a complex teacher (CatBoost) using a student logistic model built on\n",
    "SHAP-like features. CatBoost logits decompose as\n",
    "\n",
    "$$\n",
    "\\text{logit}(p_i) = \\phi_0 + \\sum_{j=1}^d \\phi_{ij}\n",
    "$$\n",
    "\n",
    "where $\\phi_{ij}$ are SHAP contributions. We fit smooth GAM functions\n",
    "$g_j(x_{ij}) \\approx \\phi_{ij}$ to map raw inputs → SHAP-like effects.\n",
    "\n",
    "To recover lost interactions, we expand these GAM features with pairwise products:\n",
    "\n",
    "$$\n",
    "z_i = [g_1(x_{i1}), \\dots, g_d(x_{id}), \\; g_j(x_{ij}) g_k(x_{ik})]\n",
    "$$\n",
    "\n",
    "and train a logistic regression:\n",
    "\n",
    "$$\n",
    "\\hat{p}_i = \\sigma\\!\\left(\\beta_0 + \\sum_j \\beta_j g_j(x_{ij}) + \\sum_{j<k} \\beta_{jk} g_j(x_{ij}) g_k(x_{ik})\\right)\n",
    "$$\n",
    "\n",
    "This student closely matches teacher Gini while remaining interpretable and deployable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "01f46713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher Gini: 0.8855\n",
      "Train Gini: 0.8825\n",
      "Test Gini : 0.8963\n",
      "Gini score (new data): 0.88\n"
     ]
    }
   ],
   "source": [
    "def fit_student_from_shap(\n",
    "    X_train,\n",
    "    shap_df,\n",
    "    X_test,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    n_splines=50,\n",
    "    include_bias=False,\n",
    "):\n",
    "    \"\"\"Fit a student model from SHAP values using GAM mappings + polynomial expansion.\"\"\"\n",
    "\n",
    "    # Fit one GAM per feature\n",
    "    gam_mappers = {}\n",
    "    for col in X_train.columns:\n",
    "        gam = LinearGAM(s(0, n_splines=n_splines)).fit(\n",
    "            X_train[[col]].values, shap_df[col].values\n",
    "        )\n",
    "        gam_mappers[col] = gam\n",
    "\n",
    "    # Transform with GAM predictions\n",
    "    def transform_with_gam(X):\n",
    "        X_new = pd.DataFrame(index=X.index)\n",
    "        for col in X.columns:\n",
    "            X_new[col] = gam_mappers[col].predict(X[[col]].values)\n",
    "        return X_new\n",
    "\n",
    "    X_train_shap = transform_with_gam(X_train)\n",
    "    X_test_shap = transform_with_gam(X_test)\n",
    "\n",
    "    # Polynomial expansion\n",
    "    poly = PolynomialFeatures(include_bias=include_bias, interaction_only=True)\n",
    "    X_train_poly = poly.fit_transform(X_train_shap)\n",
    "    X_test_poly = poly.transform(X_test_shap)\n",
    "\n",
    "    # Logistic regression on expanded features\n",
    "    lr_student = LogisticRegression()\n",
    "    lr_student.fit(X_train_poly, y_train)\n",
    "\n",
    "    # Predictions & Gini\n",
    "    y_train_pred = lr_student.predict_proba(X_train_poly)[:, 1]\n",
    "    y_test_pred = lr_student.predict_proba(X_test_poly)[:, 1]\n",
    "\n",
    "    return {\n",
    "        \"gam_mappers\": gam_mappers,\n",
    "        \"poly\": poly,\n",
    "        \"lr_student\": lr_student,\n",
    "        \"train_gini\": gini_score(y_train, y_train_pred),\n",
    "        \"test_gini\": gini_score(y_test, y_test_pred),\n",
    "    }\n",
    "\n",
    "\n",
    "def score_new_data_with_shap_gam(X_new, artifacts):\n",
    "    \"\"\"\n",
    "    Apply trained student (GAM + Poly + Logistic Regression) to new raw data.\n",
    "    \"\"\"\n",
    "    # 1. Transform raw X with GAM mappings\n",
    "    X_new_shap = pd.DataFrame(index=X_new.index)\n",
    "    for col in X_new.columns:\n",
    "        X_new_shap[col] = artifacts[\"gam_mappers\"][col].predict(X_new[[col]].values)\n",
    "\n",
    "    # 2. Apply polynomial feature expansion (if used)\n",
    "    X_new_poly = artifacts[\"poly\"].transform(X_new_shap)\n",
    "\n",
    "    # 3. Predict with student logistic regression\n",
    "    return artifacts[\"lr_student\"].predict_proba(X_new_poly)[:, 1]\n",
    "\n",
    "\n",
    "# Train teacher\n",
    "teacher = cb.CatBoostClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=6,\n",
    "    allow_writing_files=False,\n",
    "    verbose=False,\n",
    ")\n",
    "teacher.fit(X_train, y_train)\n",
    "\n",
    "teacher_gini = gini_score(y_train, teacher.predict_proba(X_train)[:, 1])\n",
    "print(f\"Teacher Gini: {teacher_gini:.4f}\")\n",
    "\n",
    "# Compute SHAP values once\n",
    "shap_values = teacher.get_feature_importance(\n",
    "    type=\"ShapValues\", data=cb.Pool(X_train, y_train)\n",
    ")\n",
    "shap_df = pd.DataFrame(\n",
    "    shap_values[:, :-1], index=X_train.index, columns=X_train.columns\n",
    ")\n",
    "\n",
    "# Run student training with polynomial expansion\n",
    "results = fit_student_from_shap(\n",
    "    X_train,\n",
    "    shap_df,\n",
    "    X_test,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    n_splines=30,\n",
    ")\n",
    "\n",
    "print(\"Train Gini:\", round(results[\"train_gini\"], 4))\n",
    "print(\"Test Gini :\", round(results[\"test_gini\"], 4))\n",
    "\n",
    "# Inference on unseen data\n",
    "X_new = X_test.sample(100, random_state=42)\n",
    "y_new = y_test.loc[X_new.index]\n",
    "y_new_pred = score_new_data_with_shap_gam(X_new, results)\n",
    "\n",
    "gini_new = gini_score(y_new, y_new_pred)\n",
    "print(\"Gini score (new data):\", round(gini_new, 4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
